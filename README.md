# ğŸï¸ F1 Pit Stop Strategy Simulator

An advanced, interactive simulator built with Python, Streamlit, and Reinforcement Learning (`Q-learning` & `PPO`) to explore and optimize Formula 1 pit stop strategies.

This application simulates full F1 races, considering dynamic conditions including non-linear tire degradation, track characteristics, fuel load, traffic, probabilistic weather (rain), and on-track incidents like Safety Cars and Virtual Safety Cars (VSC).

---

## âœ¨ Key Features

### Dynamic Race Environment
A custom-built environment using `gymnasium` that simulates:

- Lap-by-lap race progression with track-specific base lap times.
- **Advanced Tire Model**: Non-linear degradation profiles for multiple compounds (Soft, Medium, Hard) and wet-weather tires (Intermediate, Wet), each with unique wear rates and performance drop-offs.
- **Fuel consumption** and its impact on lap times.
- Variable **track grip** and **temperature evolution**.
- **Randomized traffic** events affecting lap times.

### Intelligent Strategy Agents

- **Q-Learning Agent**: A custom-built agent that learns optimal pit strategies by discretizing the complex state space.
- **PPO Agent**: Utilizes Proximal Policy Optimization from `stable-baselines3` for more advanced strategy learning.
- **Expanded Action Space**: Agents can make granular decisions, choosing not just when to pit, but which specific tire compound to switch to.

### Comprehensive Streamlit Dashboard

A rich, interactive UI to:

- Configure detailed race parameters (laps, track selection, driver profiles).
- Define complex race scenarios with probabilistic rain forecasts and scheduled Safety Car periods.
- Run simulations using different strategies: **Q-learning**, **PPO**, **Head-to-Head**, and **Custom**.
- **Statistical Comparison Mode**: Run batch simulations (e.g., 50 races per strategy) to robustly compare the performance distributions of different agents using summary tables and box plots.

### Advanced Data Visualization (with Plotly)

- Real-time animated lap metrics (tire wear, fuel, traffic).
- Strategic Event Timeline with emoji markers: ğŸ…¿ï¸ Pits, ğŸŒ§ï¸ Rain, âš ï¸ SC, ğŸš¦ VSC.
- Detailed post-race analysis charts (lap time delta, tire usage, track conditions).

### ML-Powered Insights & Reporting

- Integrates a separately trained model (`lap_time_predictor.pkl`) to predict lap times based on race conditions, offering a comparison with the simulation's actual outcomes.
- Generates **downloadable PDF race summary reports**.

---

## ğŸ› ï¸ Technologies Used

- **Core**: Python 3.10+
  
### Simulation Environment
- `gymnasium`
- `NumPy`
- `Pandas`

### Reinforcement Learning
- Custom Q-learning implementation
- `stable-baselines3[extra]` for PPO
- `torch` (backend for SB3)

### Machine Learning (Lap Time Predictor)
- `scikit-learn` (RandomForestRegressor)
- `joblib`

### User Interface & Visualization
- `Streamlit`
- `Plotly`
- `Matplotlib`
- `Seaborn`

### PDF Generation
- `fpdf2`


---

## ğŸ§± Project Structure
```bash
pit-stop-simulator/
â”‚
â”œâ”€â”€ main.py                      # Q-learning training loop + pit heatmap
â”œâ”€â”€ ppo_train.py                 # Train PPO agent using Stable-Baselines3
â”œâ”€â”€ ppo_eval.py                  # Evaluate PPO agent and track pit stops
â”œâ”€â”€ compare_strategies.py        # Visual side-by-side comparison of pit strategies
â”œâ”€â”€ streamlit_app.py             # Interactive Streamlit UI with race events and agent simulations
â”‚
â”œâ”€â”€ env/
â”‚   â””â”€â”€ gym_race_env.py          # Custom OpenAI Gymnasium race simulation environment
â”‚
â”œâ”€â”€ rl/
â”‚   â””â”€â”€ q_learning_agent.py      # Q-learning agent logic
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ppo_pit_stop.zip         # Saved PPO model
â”‚
â”œâ”€â”€ saved_agents/                # Saved Q-learning agents (per team/profile)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ q_learning_pit_decisions.npy   # Pit stop logs from Q-learning
â”‚   â””â”€â”€ ppo_pit_decisions.npy          # Pit stop logs from PPO
â”‚
â”œâ”€â”€ requirements.txt             # Python package dependencies
â””â”€â”€ README.md                    # Project overview and setup instructions
```
---

## ğŸš€ How It Works

- **Environment** simulates 58 laps with dynamic traffic and tire wear
- **Actions**: `0 = stay out`, `1 = pit`
- **Observations**: `[lap number, tire wear, traffic level]`
- **Rewards**: Negative lap times (shorter = better)

---

## ğŸ¤– Agent Logic (Q-learning)

- Discretizes continuous state space (lap, tire, traffic)
- Balances exploration vs. exploitation using Îµ-greedy strategy
- Updates Q-values over episodes to learn optimal pit timing

---

## ğŸ“Š Visualization

After training, a reward curve is plotted showing the agent's learning progress over time.

---

## ğŸ§ª Streamlit Simulation Dashboard

The interactive UI lets you:

- ğŸ” Replay Animated Laps with ghost trails
- ğŸ“Š Visualizations: Tire wear, traffic, fuel, grip, temperature
- ğŸŒ§ï¸ Probabilistic Rain Forecasts by lap range & chance %
- ğŸ› Tire Compounds: Soft, Medium, Hard, Intermediate, Wet
- ğŸš¨ Dynamic Events: Safety Cars, VSCs, Crashes
- ğŸ§  Driver Profiles: Aggressive, Balanced, Conservative
- ğŸ› ï¸ Custom Strategy Mode: Pick your own pit stops
- ğŸ“ˆ ML Lap Time Predictor: Trained with XGBoost
- ğŸ§¾ PDF Race Summary Reports for download
- ğŸ§  Strategic Timeline with emoji markers:
- ğŸ…¿ï¸ Pit Stop
- ğŸŒ§ï¸ Rain
- âš ï¸ Safety Car

---

## ğŸ“¦ Install & Run

```bash
# Install dependencies
pip install -r requirements.txt

# Run the training script
python3 main.py

# Launch the Streamlit simulator
streamlit run streamlit_app.py
```

---

## ğŸ› ï¸ Roadmap

- Visualize pit stop decisions (lap heatmaps)
- Add multiple tire compounds
- Introduce safety car periods or weather
- Switch to PPO (for continuous observations)
- Add real-time event adaptation
- Support persistent driver learning and history tracking
- Enable full race replay controls

 ---
 
## ğŸ“„ License

MIT â€” use, modify, and share freely.

---

ğŸ™Œ Credits

Built with:

- SimPy
- OpenAI Gym
- NumPy
- Matplotlib
