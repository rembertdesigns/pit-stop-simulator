# üèéÔ∏è F1 Pit Stop Strategy Simulator
[**Launch Simulator**](https://pit-stop-sim.streamlit.app/)

<img 
  width="1536" 
  height="1024" 
  alt="F1 Pit Stop Strategy Simulator - AI-powered Formula 1 race strategy optimization tool with reinforcement learning for tire management and pit timing decisions" 
  title="F1 Pit Stop Strategy Simulator - AI Formula 1 Race Strategy Optimization Tool"
  src="https://github.com/user-attachments/assets/f61b6403-1633-4b13-83ab-ccdff35568dd"
  loading="lazy"
/>

An advanced, interactive Formula 1 pit stop strategy simulator leveraging **Reinforcement Learning** and **Machine Learning** to optimize race strategies through dynamic simulation. This project demonstrates AI-driven decision-making for pit stop timing and tire compound selection in realistic race environments.

---

## üìã Table of Contents
- [Overview](#-overview)
- [Core Innovation](#-core-innovation)
- [Key Features](#-key-features)
- [Technical Architecture](#-technical-architecture)
- [Installation](#-installation)
- [Model Training Pipeline](#-model-training-pipeline)
- [Usage Guide](#-usage-guide)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Performance Metrics](#-performance-metrics)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## üéØ Overview

This simulator creates a high-fidelity F1 race environment where AI agents learn optimal pit stop strategies through reinforcement learning. The system models complex race dynamics including:

- **Race Dynamics Simulation**
  - **Tire Degradation Models:** Non-linear wear patterns with compound-specific coefficients  
    - Soft: High grip, rapid degradation (1.5x base rate)
    - Medium: Balanced performance (1.0x base rate)
    - Hard: Durability focus (0.7x base rate)
    - Intermediate/Wet: Weather-dependent characteristics

- **Environmental Factors**
  - Dynamic weather system with probabilistic rain forecasts
  - Track temperature variations (20-50¬∞C range)
  - Grip factor evolution based on rubber buildup and conditions

- **Race Incidents**
  - Safety Car (SC) deployments with configurable duration
  - Virtual Safety Car (VSC) periods
  - Strategic opportunity windows during caution periods

- **Performance Variables**
  - Traffic modeling with time penalties (1-10s per lap)
  - Fuel consumption effects on lap times (~0.03s per kg)
  - FIA regulation compliance (mandatory compound usage)

  ---

## üß† Core Innovation

### Dual-Agent Architecture

The project implements two complementary reinforcement learning approaches:

1. **Q-Learning Agent**
   - **Algorithm:** Tabular Q-Learning with discretized state space
   - **State Space Discretization:**
```python
State = (
    lap_bucket,           # 0-3 (race phase)
    tire_wear_bucket,     # 0-9 (0-100% in 10% increments)
    traffic_bucket,       # 0-2 (low/medium/high)
    rain_status,          # 0-1 (binary)
    safety_car_status,    # 0-1 (binary)
    vsc_status           # 0-1 (binary)
)
```
**Action Space:** 6 discrete actions

- **Action 0:** Stay out (no pit)
- **Actions 1-5:** Pit for specific compound (Soft / Medium / Hard / Intermediate / Wet)

**Learning Parameters:**

- **Learning rate (Œ±):** 0.1
- **Discount factor (Œ≥):** 0.99
- **Epsilon-greedy exploration:** Œµ = 1.0 ‚Üí 0.01 (decay over 2000 episodes)

2. **PPO Agent (Proximal Policy Optimization)**
  - Framework: Stable-Baselines3 implementation
  - Observation Space: 7-dimensional continuous vector
```python
obs = [
    current_lap / total_laps,        # Normalized race progress
    tire_wear / 100.0,               # Normalized tire condition
    traffic,                         # Traffic intensity [0, 1]
    fuel_weight / initial_fuel,      # Normalized fuel load
    rain_intensity,                  # Rain strength [0, 1]
    safety_car_active,               # Binary flag
    vsc_active                       # Binary flag
]
```
**Network Architecture**

- Policy Network: MLP with 2 hidden layers (64 neurons each)
- Value Network: Shared feature extraction  
- Activation: Tanh

**Training Hyperparameters**

- Total timesteps: 300,000  
- Batch size: 64  
- Learning rate: 2.5e-4  
- GAE Œª: 0.95  
- Clip range: 0.2

---

## ‚ú® Key Features

### ü§ñ AI Strategy Agents

**Q-Learning Agent**
- Fast inference (~0.1ms per decision)  
- Interpretable state-action mappings  
- Optimal for discrete, well-defined scenarios  
- 15 agents (5 teams √ó 3 driver profiles)

**PPO Agent**
- Continuous learning capability  
- Superior generalization to novel conditions  
- Neural network-based policy approximation  
- Single model handles all team/profile combinations

**Performance Comparison**
- Head-to-Head mode for direct agent comparison  
- Statistical analysis over 100+ race simulations  
- Distribution plots for race time and pit stop counts

### Race Simulation Engine

**Session Types**
1. **Practice:** 3 stints √ó 3 laps, random tire compounds  
2. **Qualifying:** 3 flying laps, best time recording  
3. **Race:** Full distance with strategic pit stops  
4. **Full Weekend:** Complete P ‚Üí Q ‚Üí R progression  
5. **Statistical Comparison:** Batch simulations (10‚Äì100 runs)

**Circuit Library:** 9 Pre-Configured Tracks
| Track       | Pit Time | Wear Rate | Traffic | Base Lap |
|--------------|-----------|-----------|----------|-----------|
| Monza        | 28s       | 1.1x      | 3.0s     | 80.0s     |
| Spa          | 32s       | 1.2x      | 4.0s     | 105.0s    |
| Monaco       | 25s       | 1.4x      | 7.5s     | 71.0s     |
| Bahrain      | 30s       | 2.0x      | 5.0s     | 92.0s     |
| Silverstone  | 29s       | 1.8x      | 4.5s     | 88.0s     |

### üìä Analytics Dashboard
**Real-Time Visualizations** (Plotly-powered)
- Animated lap-by-lap metrics with **0.001‚Äì0.5s replay speed**  
- Tire wear progression with **rolling average smoothing**  
- **Traffic intensity heatmaps**  
- **Fuel consumption curves**

**Post-Race Analysis**
- **Lap time delta charts** (vs. first valid lap baseline)  
- **Strategic event timeline** with emoji markers (üÖøÔ∏è üåßÔ∏è ‚ö†Ô∏è üö¶)  
- **Tire compound usage distribution**  
- **Track temperature and grip factor evolution**

**ML Insights**
- **RandomForest lap time predictions** vs. actual performance  
- **Feature importance rankings**  
- **Prediction error analysis** (RMSE, R¬≤)

### üß† Machine Learning Pipeline
**Lap Time Predictor**
- **Model:** `RandomForestRegressor (150 trees)`  
- **Features:** 14 inputs (7 numeric + 5 tire compound dummies + 2 boolean)  
- **Training data:** Aggregated from simulation logs  
- **Typical performance:** R¬≤ > 0.85, RMSE < 2.0s

---

## üõ†Ô∏è Technical Architecture

### Technology Stack
<img width="421" height="433" alt="Screenshot 2025-10-06 at 2 39 09‚ÄØPM" src="https://github.com/user-attachments/assets/24571e4c-2d1b-4934-bcfc-1597e2fb8fa6" />

### Core Dependencies

| Package           | Version | Purpose                        |
|--------------------|----------|--------------------------------|
| **Python**         | 3.10+    | Base runtime                   |
| **Streamlit**      | 1.28+    | Web application framework      |
| **Gymnasium**      | 0.29+    | RL environment standard        |
| **Stable-Baselines3** | 2.1+ | PPO implementation             |
| **PyTorch**        | 2.0+     | Deep learning backend          |
| **Scikit-learn**   | 1.3+     | ML predictor model             |
| **Plotly**         | 5.17+    | Interactive visualizations     |
| **NumPy**          | 1.24+    | Numerical computing            |
| **Pandas**         | 2.0+     | Data manipulation              |

---

## üöÄ Installation

### Prerequisites
- Python 3.10 or newer  
- Git  
- ~2GB disk space for models and dependencies  
- 4GB+ RAM recommended for PPO training  

### Quick Setup
1. **Clone Repository**
```bash
git clone https://github.com/rembertdesigns/pit-stop-simulator.git
cd pit-stop-simulator
```
2. **Create Virtual Environment** (Recommended)
```bash
# macOS/Linux
python3 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```
3. **Install Dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```
4. **Verify Installation**
```bash
python -c "import streamlit; import gymnasium; import stable_baselines3; print('‚úÖ All dependencies installed')"
```
---

## üéì Model Training Pipeline

### ‚ö†Ô∏è Important Notice
Pre-trained models are **not included** in the repository. You must train them yourself using the provided scripts.

### Training Workflow
```bash
1. Generate Data ‚Üí 2. Train ML Predictor ‚Üí 3. Train Q-Agents ‚Üí 4. Train PPO Agent
```
### Step 1: Generate Initial Data

Run 2-3 race simulations to create training data:
```bash
streamlit run streamlit_app.py
```
**Action Items**:
- Navigate to the app (http://localhost:8501)
- Configure race settings (50+ laps recommended)
- Run simulations with varied conditions (rain, SC, different tracks)
- Verify `logs/gym_race_lap_data.csv` contains data (should have 100+ rows)

### Step 2: Train Lap Time Predictor
```bash
python train_lap_model.py
```
**What This Does**:
- Loads data from `logs/gym_race_lap_data.csv`
- Preprocesses features (one-hot encoding, normalization)
- Trains RandomForestRegressor
- Saves model to `models/lap_time_predictor.pkl`
- Prints evaluation metrics (RMSE, R¬≤, feature importances)

**Expected Output**:
```bash
Training set size: 800 samples, Test set size: 200 samples
Training RandomForestRegressor model with 14 features...
Model training complete.

--- Model Evaluation on Test Set ---
  Root Mean Squared Error (RMSE): 1.847
  R-squared (R2 Score):           0.891

‚úÖ Model retrained and saved to: models/lap_time_predictor.pkl
```
### Step 3: Train Q-Learning Agents
```
python main.py
```
**Configuration** (in `main.py`):
```python
TEAMS_TO_TRAIN = ["Ferrari", "Red Bull", "Mercedes", "McLaren", "Aston Martin"]
PROFILES_TO_TRAIN = ["Aggressive", "Balanced", "Conservative"]
TRAINING_EPISODES = 2000
```
**What This Does**:
- Creates 15 agents (5 teams √ó 3 profiles)
- Trains each agent for 2000 episodes (~30 min on modern CPU)
- Saves agents to `saved_agents/{Team}_{Profile}_q.pkl`
- Generates training plots in `training_figures/`

**Expected Output** (per agent):
```bash
--- Initializing training for: Ferrari - Aggressive ---
Training Ferrari Aggressive: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [05:42<00:00, 5.84it/s]
--- Training for Ferrari - Aggressive complete ---
‚úÖ Agent successfully saved to: saved_agents/Ferrari_Aggressive_q.pkl
```
### Step 4: Train PPO Agent
```bash
python train_ppo.py
```
**What This Does**:
- Creates vectorized training environment
- Trains PPO agent for 300,000 timesteps (~2-3 hours on CPU, ~30 min on GPU)
- Saves checkpoints every 10,000 steps
- Evaluates and saves best model
- Final model saved to `models/ppo_pit_stop.zip`

**Expected Output**:
```bash
Creating training environment...
PPO Model Created. Observation Space: Box(7,), Action Space: Discrete(6)
Starting PPO training for 300000 timesteps...
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 58.0     |
|    ep_rew_mean     | -5247.32 |
| time/              |          |
|    fps             | 1247     |
|    iterations      | 146      |
|    time_elapsed    | 240      |
|    total_timesteps | 299008   |
...
‚úÖ Training complete. Final model saved to: models/ppo_pit_stop.zip
```
### Verifying Training Success

After training, verify all models exist:
```bash
ls -lh models/
# Should show:
# lap_time_predictor.pkl
# ppo_pit_stop.zip

ls -lh saved_agents/
# Should show 15 .pkl files for Q-agents
```

---

## üìñ Usage Guide

## Launch Application
```bash
bashstreamlit run streamlit_app.py
```
Access at: `http://localhost:8501`

### Basic Simulation Workflow

1. **Select Strategy** (Sidebar)
   - Q-Learning: Uses trained Q-table agent
   - PPO: Uses neural network policy
   - Custom: Manual pit lap selection

2. **Configure Race Parameters**
   - Total Laps: 20-80 (default: 58)
   - Track: Select from 9 circuits or define custom
   - Team & Driver Profile: Affects pit thresholds and tire wear

3. **Add Race Events** (Optional)
   - Rain Forecast: Define probability windows
   - Safety Car: Select specific laps
   - Initial Tire: Soft/Medium/Hard

4. **Run Simulation**
   - Click "‚ñ∂Ô∏è Start Simulation"
   - Watch animated lap metrics
   - Review post-race analytics

5. **Analyze Results**
   - Lap time deltas
   - Strategic event timeline
   - ML predictions vs. actual
   - Download PDF report

### Advanced Features

#### Head-to-Head Mode
Compare Q-Learning vs. PPO directly:
- Session Type: "Head-to-Head"
- Side-by-side race visualization
- Comparative summary table

#### Statistical Comparison
Robust strategy evaluation:
- Session Type: "Statistical Comparison"
- Select 2-3 strategies
- Configure 10-100 runs per strategy
- View distribution plots (box plots, histograms)

#### Full Weekend Mode
Experience P ‚Üí Q ‚Üí R progression:
- Session Type: "Full Weekend"
- Simulates Practice (9 laps), Qualifying (3 laps), Race (full distance)
- Carries strategic insights between sessions

---

## üìÅ Project Structure
```bash
pit-stop-simulator/
‚îÇ
‚îú‚îÄ‚îÄ streamlit_app.py              # üñ•Ô∏è  Main web application (3500+ lines)
‚îÇ   ‚îú‚îÄ‚îÄ Model downloader (Hugging Face Hub)
‚îÇ   ‚îú‚îÄ‚îÄ Simulation orchestration
‚îÇ   ‚îú‚îÄ‚îÄ Plotly visualizations
‚îÇ   ‚îî‚îÄ‚îÄ PDF report generation
‚îÇ
‚îú‚îÄ‚îÄ env/
‚îÇ   ‚îî‚îÄ‚îÄ gym_race_env.py           # üèÅ Gymnasium-compliant F1 environment
‚îÇ       ‚îú‚îÄ‚îÄ State space definition
‚îÇ       ‚îú‚îÄ‚îÄ Reward function (lap time + penalties)
‚îÇ       ‚îú‚îÄ‚îÄ Weather/SC/VSC logic
‚îÇ       ‚îî‚îÄ‚îÄ Tire/fuel degradation models
‚îÇ
‚îú‚îÄ‚îÄ rl/
‚îÇ   ‚îî‚îÄ‚îÄ q_learning_agent.py       # üß† Tabular Q-Learning implementation
‚îÇ       ‚îú‚îÄ‚îÄ State discretization (6D ‚Üí buckets)
‚îÇ       ‚îú‚îÄ‚îÄ Epsilon-greedy exploration
‚îÇ       ‚îî‚îÄ‚îÄ Q-table updates (Bellman equation)
‚îÇ
‚îú‚îÄ‚îÄ train_ppo.py                  # üöÇ PPO agent training script
‚îÇ   ‚îú‚îÄ‚îÄ Vectorized environment setup
‚îÇ   ‚îú‚îÄ‚îÄ Hyperparameter configuration
‚îÇ   ‚îú‚îÄ‚îÄ Checkpoint/eval callbacks
‚îÇ   ‚îî‚îÄ‚îÄ TensorBoard logging
‚îÇ
‚îú‚îÄ‚îÄ train_lap_model.py            # üìà ML lap time predictor training
‚îÇ   ‚îú‚îÄ‚îÄ CSV data loading/preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ RandomForest training
‚îÇ   ‚îú‚îÄ‚îÄ Feature engineering (one-hot encoding)
‚îÇ   ‚îî‚îÄ‚îÄ Model evaluation (RMSE, R¬≤)
‚îÇ
‚îú‚îÄ‚îÄ main.py                       # üéØ Batch Q-Learning agent training
‚îÇ   ‚îú‚îÄ‚îÄ Multi-agent training loop (15 agents)
‚îÇ   ‚îú‚îÄ‚îÄ Progress tracking (tqdm)
‚îÇ   ‚îú‚îÄ‚îÄ Training visualization (rewards, heatmaps)
‚îÇ   ‚îî‚îÄ‚îÄ Agent persistence (pickle)
‚îÇ
‚îú‚îÄ‚îÄ ppo_eval.py                   # üìä PPO agent evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ Deterministic policy rollouts
‚îÇ   ‚îú‚îÄ‚îÄ Detailed lap-by-lap logging
‚îÇ   ‚îî‚îÄ‚îÄ Episode statistics
‚îÇ
‚îú‚îÄ‚îÄ models/                       # ü§ñ Trained models (auto-downloaded from HF Hub)
‚îÇ   ‚îú‚îÄ‚îÄ ppo_pit_stop.zip          # PPO neural network policy
‚îÇ   ‚îî‚îÄ‚îÄ lap_time_predictor.pkl    # RandomForest regressor
‚îÇ
‚îú‚îÄ‚îÄ saved_agents/                 # üíæ Q-Learning agents (15 .pkl files)
‚îÇ   ‚îú‚îÄ‚îÄ Ferrari_Aggressive_q.pkl
‚îÇ   ‚îú‚îÄ‚îÄ Mercedes_Balanced_q.pkl
‚îÇ   ‚îî‚îÄ‚îÄ ... (13 more combinations)
‚îÇ
‚îú‚îÄ‚îÄ logs/                         # üìù Simulation data (auto-generated)
‚îÇ   ‚îî‚îÄ‚îÄ gym_race_lap_data.csv     # Lap-by-lap race logs
‚îÇ
‚îú‚îÄ‚îÄ training_figures/             # üìä Training visualizations (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ rewards_{Team}_{Profile}.png
‚îÇ   ‚îî‚îÄ‚îÄ heatmap_{Team}_{Profile}.png
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # üì¶ Python dependencies
‚îú‚îÄ‚îÄ README.md                     # üìö This documentation
‚îî‚îÄ‚îÄ LICENSE                       # ‚öñÔ∏è  MIT License
```

---

## ‚öôÔ∏è Configuration

### Environment Parameters
**Track Configuration** (`PitStopEnv` initialization):
```python
env = PitStopEnv(
    total_laps=58,                  # Race distance
    base_lap_time_seconds=90.0,     # Ideal dry lap time
    pit_time=28,                    # Pit stop time loss
    tire_wear_rate_config=1.1,      # Track abrasiveness multiplier
    traffic_penalty_config=3.0      # Time lost per traffic unit
)
```
### Driver Profile Effects

Profiles modify agent behavior and physics:

| Profile | Pit Threshold | Tire Wear Multiplier | Overtake Bonus |
|---------|---------------|----------------------|----------------|
| **Aggressive** | 75% wear | 1.15√ó (faster degradation) | -0.2s (time gain) |
| **Balanced** | 65% wear | 1.0√ó (baseline) | 0.0s (neutral) |
| **Conservative** | 55% wear | 0.85√ó (slower degradation) | +0.1s (time loss) |

### Weather System

**Rain Forecast Configuration**:
```python
rain_forecast_ranges = [
    {
        "start": 15,           # Lap window start
        "end": 25,             # Lap window end
        "probability": 0.7,    # 70% chance of rain
        "intensity": 0.6       # Rain strength (0.0-1.0)
    }
]
```
**Rain Effects:**
- Lap time penalty: +5-15s (intensity-dependent)
- Grip reduction: -30% to -60%
- Forces Intermediate/Wet tire compound

---

## üìä Performance Metrics

### Agent Evaluation Metrics
**Total Reward:** Cumulative race performance
```bash
Total Reward = Œ£(lap_rewards) - penalties
where lap_reward = -(lap_time) + bonuses
```
**Pit Efficiency Rating:** Time optimization estimate
```bash
Pit Efficiency = 100 √ó (1 - (pit_stops √ó pit_time) / total_race_time)
```
**FIA Penalties**:
- Missing 2-compound rule (dry races): -20 units
- Unsafe release: -10 units (future feature)

### Typical Performance Ranges

| Metric | Q-Learning | PPO | Custom |
|--------|------------|-----|--------|
| Total Reward | -5500 to -4800 | -5300 to -4700 | -6000 to -5000 |
| Avg Pit Stops | 1-3 | 1-2 | User-defined |
| Pit Efficiency | 85-92% | 88-95% | 70-95% |

### ML Model Accuracy

**Lap Time Predictor**:
- RMSE: 1.5-2.5 seconds
- R¬≤: 0.85-0.92
- Feature Importance: tire_wear (35%), lap (18%), fuel_weight (14%)

---

## üõ£Ô∏è Roadmap

### Phase 1: Multi-Agent Racing (Q3 2025)
- [ ] Implement 20-car grid simulation
- [ ] Inter-agent competition with overtaking logic
- [ ] Position-based reward shaping
- [ ] Qualifying-based grid positions

### Phase 2: Advanced Strategy (Q4 2025)
- [ ] Dynamic pit strategy updates (mid-race replanning)
- [ ] "What-If" scenario analysis tool
- [ ] Tire compound optimizer (pre-race selection)
- [ ] Undercut/overcut detection system

### Phase 3: Enhanced Realism (Q1 2026)
- [ ] Red flag handling and race restarts
- [ ] Driver skill variation (consistency, errors)
- [ ] Team radio communication simulation
- [ ] Pit crew performance modeling

### Phase 4: Deep Learning (Q2 2026)
- [ ] Transformer-based strategy predictor
- [ ] LSTM for lap time forecasting
- [ ] Convolutional networks for track layout analysis
- [ ] Multi-agent reinforcement learning (MARL)

### Phase 5: Championship Mode (Q3 2026)
- [ ] Full season simulation (23 races)
- [ ] Points system and standings
- [ ] Car development progression
- [ ] Strategic resource allocation (tire allocation)

---

## üîó External Resources

### Model Repository
All trained models hosted on [Hugging Face Hub](https://huggingface.co/Richard1224/pit-stop-simulator-models)

**Automatic Download**: Models are fetched on first app launch via `download_models_from_hf()` function.

**Manual Download** (if needed):
```bash
huggingface-cli download Richard1224/pit-stop-simulator-models --local-dir ./
```
### Documentation
- **Stable-Baselines3**: [RL algorithms documentation](https://stable-baselines3.readthedocs.io/)
- **Gymnasium**: [Environment API reference](https://gymnasium.farama.org/)
- **Streamlit**: [Component library](https://docs.streamlit.io/)

---

## üôå Contributing

Contributions welcome! Priority areas:

### High-Impact Improvements
1. **Real F1 Data Integration**: Parse actual race telemetry (F1 API, FastF1 library)
2. **Additional RL Algorithms**: A3C, SAC, TD3 implementations
3. **Performance Optimization**: Numba JIT compilation, parallel simulation
4. **Track Expansion**: Add 2024 F1 calendar circuits with accurate characteristics

### Development Setup
```bash
# Fork and clone
git clone https://github.com/YOUR_USERNAME/pit-stop-simulator.git
cd pit-stop-simulator

# Create feature branch
git checkout -b feature/your-feature-name

# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests (if implemented)
pytest tests/

# Submit PR with detailed description
```
### Code Standards
- Follow PEP 8 style guide
- Add docstrings to new functions
- Include type hints for function signatures
- Update README if adding major features

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see [LICENSE](LICENSE) file for details.
```bash
MIT License - Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

## üèÜ Acknowledgments

Built with passion for Formula 1 and powered by:

- **[Streamlit](https://streamlit.io/)** ‚Äî Rapid data app development framework
- **[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)** ‚Äî Production-ready RL implementations
- **[Gymnasium](https://gymnasium.farama.org/)** ‚Äî Industry-standard RL environment API
- **[Plotly](https://plotly.com/python/)** ‚Äî Interactive visualization library
- **[Scikit-learn](https://scikit-learn.org/)** ‚Äî Machine learning fundamentals
- **[Hugging Face Hub](https://huggingface.co/)** ‚Äî Model hosting and distribution

### Inspiration
This project honors the strategic complexity and split-second decisions that define Formula 1 as the pinnacle of motorsport. Every pit stop, tire choice, and weather call can mean the difference between victory and defeat.

---

<div align="center">

**[‚¨Ü Back to Table of Contents](#-table-of-contents)**

Made with ‚ù§Ô∏è for F1 fans and ML enthusiasts

[Report Bug](https://github.com/rembertdesigns/pit-stop-simulator/issues) ¬∑ [Request Feature](https://github.com/rembertdesigns/pit-stop-simulator/issues) ¬∑ [Documentation](https://github.com/rembertdesigns/pit-stop-simulator/wiki)

</div>


